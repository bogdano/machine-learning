{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1CRJy7ib-pw"
   },
   "source": [
    "# Transformers and Natural Language Processing \n",
    "## *Bogdan Bošković*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKR2vc7Tcaw1"
   },
   "source": [
    "**Problem 3: Text Classification with A Large Language Model  (30 points)**  In this example you will utilize a modern large language model to classify text.  Specifically, you willse load the pre-trained BERT encoder that we discussed in class, and then fine-tune it to solve a custom text classification problem where you classify news articles into one of four categories: world, sports, business, sci/tech.   \n",
    "\n",
    "To assist with this exercise, we will need to make use of some libraries from Hugging Face, an organization that provides many widely-used libraries to support deep learning applications ([link](https://huggingface.co/)).   \n",
    "\n",
    "Below is a code skeleton for completing this task, with comments to guide you through the process of completing it. Please complete the code below and submit a pdf of your completed code with results.  *There will be a separate submission portal for this question on Moodle.  Although your code will be reviewed, you will be graded primarily based graded upon the correctness of your output*  \n",
    "\n",
    "Although the code skeleton below provides useful guidance/hints to fill in teh code, I highly recommend that you review a tutorial on text classification provided by hugging face before, or while, you complete this exercise ([tutorial link](https://huggingface.co/docs/transformers/en/tasks/sequence_classification))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxJfoQbSnvNm"
   },
   "source": [
    "**Installations:** Make sure you use pip or conda to install the following\n",
    "libraries for this exercise:  datasets, evaluate, metrics, transformers, numpy, and torch.\n",
    "\n",
    "Google Colab already has torch and numpy, but you will still need to install\n",
    "transformers, datasets, evaluate and metrics.  You can copy and paste the line below into colab and it will install them.\n",
    "\n",
    "*pip install transformers datasets evaluate accelerate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igpB1LqdbM7B"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7600/7600 [00:00<00:00, 13426.69 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Necessary Imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Load the AG News dataset using load_dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Define a function to tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# TODO: Tokenize the training and testing data. Hint: use .map to apply the tokenize function above to your train and test datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load TinyBERT model We use TinyBERT, which requires substantially less\n",
    "# compute than BERT, with only a modest reduction in accuracy\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1Rc6DZWDbM7D"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',             # output directory\n",
    "    num_train_epochs=3,                 # number of training epochs\n",
    "    per_device_train_batch_size=8,      # batch size for training\n",
    "    per_device_eval_batch_size=16,      # batch size for evaluation\n",
    "    warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                  # strength of weight decay\n",
    "    logging_dir='./logs',               # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "MmkhVn02bM7D"
   },
   "outputs": [],
   "source": [
    "# TODO: Function to compute accuracy of the model\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy': (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "qVPqBGHwbM7E"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11250' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11250/11250 30:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.180367</td>\n",
       "      <td>0.941974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.185340</td>\n",
       "      <td>0.943421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.223086</td>\n",
       "      <td>0.945658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22308553755283356, 'eval_accuracy': 0.9456578947368421, 'eval_runtime': 9.6449, 'eval_samples_per_second': 787.982, 'eval_steps_per_second': 12.338, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# TODO: Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MFHqv9cPbM7E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Intuit gets deeper into IT, revamps Quicken The software maker adds a network management application. It also updates its Quicken personal-finance software.\"\n",
      "\n",
      "True label: Sci/Tech;   Predicted label: Sci/Tech\n",
      "\n",
      "\n",
      "\"SEVEN KILLED IN KABUL BLOODSHED At least seven people have been killed in a bomb blast in central Kabul - the second deadly explosion in Afghanistan over the weekend.\"\n",
      "\n",
      "True label: World;   Predicted label: World\n",
      "\n",
      "\n",
      "\"Bluetooth Group Outlines Strategy (NewsFactor) NewsFactor - With Bluetooth short-range wireless technology finding its way into an array of hardware products, ranging from mobile phones to in-vehicle telematics systems, a working group promoting the specification has outlined a strategy to make it even more attractive and useful.\"\n",
      "\n",
      "True label: Sci/Tech;   Predicted label: Sci/Tech\n",
      "\n",
      "\n",
      "\"Inheriting Aura From Woods, the New King of Golf Is a Lion Vijay Singh has a golf swing to envy, even when fooling around. A few days ago on the driving range at the Tour Championship, Singh grabbed Steve Flesch #39;s golf clubs.\"\n",
      "\n",
      "True label: Sports;   Predicted label: Sports\n",
      "\n",
      "\n",
      "\"Dutch security reviewed on threat THE HAGUE, Netherlands - The government vowed tough measures yesterday against what a leading politician called  quot;the arrival of jihad in the Netherlands quot; after a death threat to a Dutch lawmaker was found pinned with a knife to the body of a slain \"\n",
      "\n",
      "True label: World;   Predicted label: World\n",
      "\n",
      "\n",
      "\"US moves on to final round The United States national soccer team revealed both its immediate and long-term future in a 6-0 victory over Panama last night.\"\n",
      "\n",
      "True label: Sports;   Predicted label: Sports\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import softmax\n",
    "\n",
    "num_examples = 6\n",
    "\n",
    "def get_example(data, idx):\n",
    "    return data['text'][idx], data['label'][idx]\n",
    "\n",
    "# TODO: Make a label mapping dictionary for the AG News dataset (keys should be numbers and values should be the category as a string)\n",
    "label_map = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "\n",
    "# TODO: Select num_examples examples from the test dataset\n",
    "examples_text = []\n",
    "examples_label = []\n",
    "for i in range(num_examples):\n",
    "    text, label = get_example(test_dataset, np.random.randint(len(test_dataset)))\n",
    "    examples_text.append(text)\n",
    "    examples_label.append(label)\n",
    "\n",
    "# TODO: Tokenize the examples\n",
    "# Hint: similar to how we defined the tokenize_function above, except here you also want to set return_tensors=\"pt\"\n",
    "# to ensure that the output from the tokenizer is ready for a PyTorch model\n",
    "inputs = [tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\") for text in examples_text]\n",
    "\n",
    "# Move to the same device as model\n",
    "if torch.cuda.is_available():\n",
    "    inputs = [{k: v.cuda() for k, v in inp.items()} for inp in inputs]\n",
    "    model.cuda()\n",
    "\n",
    "# For people with a GPU on a Macintosh machine, uncomment this\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     inputs = [input.to(device) for input in inputs]\n",
    "#     device = torch.device(\"mps\")\n",
    "#     model = model.to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = [model(**inp) for inp in inputs]\n",
    "\n",
    "# TODO: Extract logits from the output and apply softmax to get probabilities\n",
    "# Hint: ModelOutput class documentation https://huggingface.co/docs/transformers/en/main_classes/output\n",
    "probabilities = [softmax(output.logits, dim=-1) for output in outputs]\n",
    "\n",
    "# Get the predicted class indices\n",
    "predicted_classes = [torch.argmax(prob, dim=-1) for prob in probabilities]\n",
    "\n",
    "# TODO: Print 6 examples where you have the example text on one line, and the true and predicted labels on the next.\n",
    "for i in range(num_examples):\n",
    "    print('\"' + examples_text[i] + '\"')\n",
    "    print(f\"\\nTrue label: {label_map[examples_label[i]]};   Predicted label: {label_map[predicted_classes[i].item()]}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
