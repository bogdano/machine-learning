{"cells":[{"cell_type":"markdown","metadata":{"id":"t9vk9Elugvmi"},"source":["# **Assignment 5: Transformers and Natural Language Processing (v1)**"]},{"cell_type":"markdown","metadata":{"id":"3JViTgNMK4_F"},"source":["## *Bogdan Bošković*\n","**NetID:** bb210898\n","\n","*Names of students you worked with on this assignment*: LIST HERE IF APPLICABLE (delete if not)\n","\n","Note: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information.\n","\n","Attributions: Portions of this assignment are adapted from the Understanding Deep Learning textbook by Prince [link here](https://udlbook.github.io/udlbook/)"]},{"cell_type":"markdown","metadata":{"id":"Y01pYhhvLffJ"},"source":["# Introduction\n","In this assignment you will implement some operations of transformers such as self-attention, and then investigate their use for natural language processing."]},{"cell_type":"markdown","metadata":{"id":"duBBZu0sLwtm"},"source":["**Problem 1: Implement Self-Attention (30 points)**  Below is an implementation of the self-attention process, which is utilized within transformers.  Please fill in the missing portions of the code, where it says \"TO DO\".  I have provided part of the correct solution at the bottom to help you determine if you are likely to be correct, but I encourage you to understand the operations well and check your code carefully to be sure!  In some cases I have added comments to help explain the operations.   \n","\n","*NOTE: you will primarily be graded upon the correctness of the output of the final tokenization routine that you need to build (i.e., the output of the last cell).*"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"OLComQyvCIJ7"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"9OJkkoNqCVK2"},"source":["The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"oAygJwLiCSri"},"outputs":[{"name":"stdout","output_type":"stream","text":["[array([[-0.41675785],\n","       [-0.05626683],\n","       [-2.1361961 ],\n","       [ 1.64027081]]), array([[-1.79343559],\n","       [-0.84174737],\n","       [ 0.50288142],\n","       [-1.24528809]]), array([[-1.05795222],\n","       [-0.90900761],\n","       [ 0.55145404],\n","       [ 2.29220801]])]\n"]}],"source":["# Set seed so we get the same random numbers\n","# DO NOT CHANGE THE RANDOM SEED!\n","np.random.seed(2)\n","# Number of inputs\n","N = 3\n","# Number of dimensions of each input\n","D = 4\n","# Create an empty list\n","all_x = []\n","# Create elements x_n and append to list\n","for n in range(N):\n","  all_x.append(np.random.normal(size=(D,1)))\n","# Print out the list\n","print(all_x)\n"]},{"cell_type":"markdown","metadata":{"id":"W2iHFbtKMaDp"},"source":["We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"79TSK7oLMobe"},"outputs":[],"source":["# Set seed so we get the same random numbers\n","# DO NOT CHANGE THE RANDOM SEED!\n","np.random.seed(1)\n","\n","# Choose random values for the parameters\n","omega_q = np.random.normal(size=(D,D))\n","omega_k = np.random.normal(size=(D,D))\n","omega_v = np.random.normal(size=(D,D))\n","beta_q = np.random.normal(size=(D,1))\n","beta_k = np.random.normal(size=(D,1))\n","beta_v = np.random.normal(size=(D,1))"]},{"cell_type":"markdown","metadata":{"id":"VxaKQtP3Ng6R"},"source":["Now let's compute the queries, keys, and values for each input"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"TwDK2tfdNmw9"},"outputs":[],"source":["# Make three lists to store queries, keys, and values\n","all_queries = []\n","all_keys = []\n","all_values = []\n","# For every input\n","for x in all_x:\n","  # TODO -- compute the keys, queries and values.\n","  # Replace these three lines with correct lines\n","  query = beta_q + omega_q @ x\n","  key = beta_k + omega_k @ x\n","  value = beta_v + omega_v @ x\n","\n","  all_queries.append(query)\n","  all_keys.append(key)\n","  all_values.append(value)"]},{"cell_type":"markdown","metadata":{"id":"Se7DK6PGPSUk"},"source":["We'll need a softmax function (equation 12.5) -- here, it will take a list of arbitrary numbers and return a list where the elements are non-negative and sum to one\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"u93LIcE5PoiM"},"outputs":[],"source":["def softmax(items_in):\n","\n","  # TO DO!!\n","  #Compute the elements of items_out\n","  # Replace this line\n","  items_out = np.exp(items_in) / np.sum(np.exp(items_in))\n","\n","  return items_out ;"]},{"cell_type":"markdown","metadata":{"id":"8aJVhbKDW7lm"},"source":["Now compute the self attention values:"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"yimz-5nCW6vQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["x_prime_0_calculated: [[1.95291253 4.36675277 4.76351526 6.26439746]]\n","x_prime_0_true: [[ 1.95291253 4.36675277  ...  ...]]\n","x_prime_1_calculated: [[ 2.46106441  2.64767206 -1.10557715 -1.1883621 ]]\n","x_prime_1_true: [[ 2.46106441  2.64767206 -1.10557715 -1.1883621 ]]\n","x_prime_2_calculated: [[1.94330266 4.45582258 4.74359057 6.28117285]]\n","x_prime_2_true: [[ 1.94330266 4.45582258 4.74359057 6.28117285]]\n"]}],"source":["# Create empty list for output\n","all_x_prime = []\n","\n","# For each output\n","for n in range(N):\n","  # Create list for dot products of query N with all keys\n","  all_km_qn = []\n","  # Compute the dot products\n","  for key in all_keys:\n","    # # TO DO!!\n","    #compute the appropriate dot product\n","    # Replace this line\n","    dot_product = all_queries[n].T @ key\n","    # Store dot product\n","    all_km_qn.append(dot_product)\n","\n","  # Compute dot product\n","  attention = softmax(all_km_qn)\n","  # Print result (should be positive sum to one)\n","  # print(\"Attentions for output \", n)\n","  # print(attention)\n","\n","  # # # TO DO!!\n","  # Compute a weighted sum of all of the values according to the attention\n","  # (equation 12.3 in UDL text)\n","  # Replace this line\n","  x_prime = np.sum([attention[n] * all_values[n] for n in range(N)], axis=0)\n","\n","  all_x_prime.append(x_prime)\n","\n","\n","# Print out true values to check you have it correct\n","# Note that I have omitted two true values for the 0th solution;\n","#  you will be graded primarily upon correctness of these two entires\n","print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n","print(\"x_prime_0_true: [[ 1.95291253 4.36675277  ...  ...]]\")\n","print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n","print(\"x_prime_1_true: [[ 2.46106441  2.64767206 -1.10557715 -1.1883621 ]]\")\n","print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n","print(\"x_prime_2_true: [[ 1.94330266 4.45582258 4.74359057 6.28117285]]\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lBKEc47sq0x6"},"source":["**Problem 2 (30 points)**  In this problem we build a \"vocabulary\" from a text string, as in figure 12.8 of the UDL book, which can be used to tokenize text for input into a neural network (e.g., transformer).   Tokenization is a step shared by nearly all forms of natural language processing (NLP), and this exercise will impart to you some familiarity with the process.  \n","\n","Work through the cells below, running each cell in turn. You will see \"TO DO\" in the comments in several blocks of code as you go; follow the instructions to complete the code in each of these instances.    In some cases I provide detailed comments to help explain some of the operations.\n","\n","*NOTE: you will primarily be graded upon the correctness of the output of the final tokenization routine that you need to build (i.e., the output of the last cell).*\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"3_WkaFO3OfLi"},"outputs":[],"source":["import re, collections"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"tVZVuauIXmJk"},"outputs":[],"source":["text =  \"a sailor went to sea sea sea \"+\\\n","        \"to see what he could see see see \"+\\\n","        \"but all that he could see see see \"+\\\n","        \"was the bottom of the deep blue sea sea sea\""]},{"cell_type":"markdown","metadata":{"id":"fF2RBrouWV5w"},"source":["**Tokenize the input sentence**\n","\n","To begin with the tokens are the individual letters and the </w> whitespace token. So, we represent each word in terms of these tokens with spaces between the tokens to delineate them.\n","\n","The tokenized text is stored in a structure that represents each word as tokens together with the count of how often that word occurs.  We'll call this the *vocabulary*."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"OfvXkLSARk4_"},"outputs":[],"source":["def initialize_vocabulary(text):\n","  # Create a dictionary with keys that are text, and values that are numberes\n","  # The keys will be our words, and the values will be how many instances of\n","  # that word that we find.\n","  vocab = collections.defaultdict(int)\n","  # Automatically split the text into a list of words, with all extra white\n","  # space removed\n","  words = text.strip().split()\n","  # Add each individual word to the dictionary, and increment count each time\n","  # we encounter that word.  We add white space to the individual letters\n","  # so that later we can apply split again and get the individual letters to\n","  # create tokens\n","  for word in words:\n","      vocab[' '.join(list(word)) + ' </w>'] += 1\n","  return vocab"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"aydmNqaoOpSm"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary: defaultdict(<class 'int'>, {'a </w>': 1, 's a i l o r </w>': 1, 'w e n t </w>': 1, 't o </w>': 2, 's e a </w>': 6, 's e e </w>': 7, 'w h a t </w>': 1, 'h e </w>': 2, 'c o u l d </w>': 2, 'b u t </w>': 1, 'a l l </w>': 1, 't h a t </w>': 1, 'w a s </w>': 1, 't h e </w>': 2, 'b o t t o m </w>': 1, 'o f </w>': 1, 'd e e p </w>': 1, 'b l u e </w>': 1})\n","Size of vocabulary: 18\n"]}],"source":["vocab = initialize_vocabulary(text)\n","print('Vocabulary: {}'.format(vocab))\n","print('Size of vocabulary: {}'.format(len(vocab)))"]},{"cell_type":"markdown","metadata":{"id":"fJAiCjphWsI9"},"source":["Find all the tokens in the current vocabulary and their frequencies"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"qYi6F_K3RYsW"},"outputs":[],"source":["def get_tokens_and_frequencies(vocab):\n","  tokens = collections.defaultdict(int)\n","  for word, freq in vocab.items():\n","      word_tokens = word.split()\n","      for token in word_tokens:\n","          ## TO DO ##\n","          ## Write a line that will update the token count in this loop\n","          ## You may use multiple lines of code, but generally this can usually\n","          ## be done with a single line of code\n","          tokens[token] += freq\n","  return tokens"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Y4LCVGnvXIwp"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens: defaultdict(<class 'int'>, {'a': 12, '</w>': 33, 's': 15, 'i': 1, 'l': 6, 'o': 8, 'r': 1, 'w': 3, 'e': 28, 'n': 1, 't': 11, 'h': 6, 'c': 2, 'u': 4, 'd': 3, 'b': 3, 'm': 1, 'f': 1, 'p': 1})\n","Number of tokens: 19\n"]}],"source":["tokens = get_tokens_and_frequencies(vocab)\n","print('Tokens: {}'.format(tokens))\n","print('Number of tokens: {}'.format(len(tokens)))"]},{"cell_type":"markdown","metadata":{"id":"_-Rh1mD_Ww3b"},"source":["Find each pair of adjacent tokens in the vocabulary\n","and count them.  We will subsequently merge the most frequently occurring pair."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"OqJTB3UFYubH"},"outputs":[],"source":["def get_pairs_and_counts(vocab):\n","    pairs = collections.defaultdict(int)\n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i in range(len(symbols)-1):\n","            ## TO DO ##\n","            ## Write a line that will update the count for\n","            ## each *pair* of symbols encountered\n","            ## You may use multiple lines of code, but this can usually\n","            ## can be done with a single line of code\n","            pairs[symbols[i], symbols[i+1]] += freq\n","    return pairs"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"d-zm0JBcZSjS"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pairs: defaultdict(<class 'int'>, {('a', '</w>'): 7, ('s', 'a'): 1, ('a', 'i'): 1, ('i', 'l'): 1, ('l', 'o'): 1, ('o', 'r'): 1, ('r', '</w>'): 1, ('w', 'e'): 1, ('e', 'n'): 1, ('n', 't'): 1, ('t', '</w>'): 4, ('t', 'o'): 3, ('o', '</w>'): 2, ('s', 'e'): 13, ('e', 'a'): 6, ('e', 'e'): 8, ('e', '</w>'): 12, ('w', 'h'): 1, ('h', 'a'): 2, ('a', 't'): 2, ('h', 'e'): 4, ('c', 'o'): 2, ('o', 'u'): 2, ('u', 'l'): 2, ('l', 'd'): 2, ('d', '</w>'): 2, ('b', 'u'): 1, ('u', 't'): 1, ('a', 'l'): 1, ('l', 'l'): 1, ('l', '</w>'): 1, ('t', 'h'): 3, ('w', 'a'): 1, ('a', 's'): 1, ('s', '</w>'): 1, ('b', 'o'): 1, ('o', 't'): 1, ('t', 't'): 1, ('o', 'm'): 1, ('m', '</w>'): 1, ('o', 'f'): 1, ('f', '</w>'): 1, ('d', 'e'): 1, ('e', 'p'): 1, ('p', '</w>'): 1, ('b', 'l'): 1, ('l', 'u'): 1, ('u', 'e'): 1})\n","Number of distinct pairs: 48\n","Most frequent pair: ('s', 'e')\n"]}],"source":["pairs = get_pairs_and_counts(vocab)\n","print('Pairs: {}'.format(pairs))\n","print('Number of distinct pairs: {}'.format(len(pairs)))\n","\n","most_frequent_pair = max(pairs, key=pairs.get)\n","print('Most frequent pair: {}'.format(most_frequent_pair))"]},{"cell_type":"markdown","metadata":{"id":"pcborzqIXQFS"},"source":["Merge the instances of the best pair in the vocabulary"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"xQI6NALdWQZX"},"outputs":[],"source":["def merge_pair_in_vocabulary(pair, vocab_in):\n","    vocab_out = {}\n","    bigram = re.escape(' '.join(pair))\n","    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n","    for word in vocab_in:\n","        word_out = p.sub(''.join(pair), word)\n","        vocab_out[word_out] = vocab_in[word]\n","    return vocab_out"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"TRYeBZI3ZULu"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary: {'a </w>': 1, 's a i l o r </w>': 1, 'w e n t </w>': 1, 't o </w>': 2, 'se a </w>': 6, 'se e </w>': 7, 'w h a t </w>': 1, 'h e </w>': 2, 'c o u l d </w>': 2, 'b u t </w>': 1, 'a l l </w>': 1, 't h a t </w>': 1, 'w a s </w>': 1, 't h e </w>': 2, 'b o t t o m </w>': 1, 'o f </w>': 1, 'd e e p </w>': 1, 'b l u e </w>': 1}\n","Size of vocabulary: 18\n"]}],"source":["vocab = merge_pair_in_vocabulary(most_frequent_pair, vocab)\n","print('Vocabulary: {}'.format(vocab))\n","print('Size of vocabulary: {}'.format(len(vocab)))"]},{"cell_type":"markdown","metadata":{"id":"bkhUx3GeXwba"},"source":["Update the tokens, which now include the best token 'se'"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"Fqj-vQWeXxQi"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens: defaultdict(<class 'int'>, {'a': 12, '</w>': 33, 's': 2, 'i': 1, 'l': 6, 'o': 8, 'r': 1, 'w': 3, 'e': 15, 'n': 1, 't': 11, 'se': 13, 'h': 6, 'c': 2, 'u': 4, 'd': 3, 'b': 3, 'm': 1, 'f': 1, 'p': 1})\n","Number of tokens: 20\n"]}],"source":["tokens = get_tokens_and_frequencies(vocab)\n","print('Tokens: {}'.format(tokens))\n","print('Number of tokens: {}'.format(len(tokens)))"]},{"cell_type":"markdown","metadata":{"id":"K_hKp2kSXXS1"},"source":["Now let's write the full tokenization routine"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"U_1SkQRGQ8f3"},"outputs":[],"source":["# TODO -- write this routine by filling in the missing parts,\n","# calling the above routines\n","def tokenize(text, num_merges):\n","  # Initialize the vocabulary from the input text\n","  vocab = initialize_vocabulary(text)\n","\n","  for i in range(num_merges):\n","    # Find the tokens and how often they occur in the vocabulary\n","    tokens = get_tokens_and_frequencies(vocab)\n","\n","    # Find the pairs of adjacent tokens and their counts\n","    pairs = get_pairs_and_counts(vocab)\n","\n","    # Find the most frequent pair\n","    most_frequent_pair = max(pairs, key=pairs.get)\n","    print('Most frequent pair: {}'.format(most_frequent_pair))\n","\n","    # Merge the code in the vocabulary\n","    vocab = merge_pair_in_vocabulary(most_frequent_pair, vocab)\n","\n","  # Find the tokens and how often they occur in the vocabulary one last time\n","  tokens = get_tokens_and_frequencies(vocab)\n","\n","  return tokens, vocab"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"w0EkHTrER_-I"},"outputs":[{"name":"stdout","output_type":"stream","text":["Most frequent pair: ('s', 'e')\n","Most frequent pair: ('e', '</w>')\n","Most frequent pair: ('a', '</w>')\n","Most frequent pair: ('se', 'e</w>')\n","Most frequent pair: ('se', 'a</w>')\n","Most frequent pair: ('t', '</w>')\n","Most frequent pair: ('h', 'e</w>')\n","Most frequent pair: ('t', 'o')\n","Most frequent pair: ('to', '</w>')\n","Most frequent pair: ('h', 'a')\n","Most frequent pair: ('ha', 't</w>')\n","Most frequent pair: ('c', 'o')\n","Most frequent pair: ('co', 'u')\n","Most frequent pair: ('cou', 'l')\n","Most frequent pair: ('coul', 'd')\n","Most frequent pair: ('could', '</w>')\n","Most frequent pair: ('t', 'he</w>')\n","Most frequent pair: ('s', 'a')\n","Most frequent pair: ('sa', 'i')\n","Most frequent pair: ('sai', 'l')\n","Most frequent pair: ('sail', 'o')\n","Most frequent pair: ('sailo', 'r')\n"]}],"source":["tokens, vocab = tokenize(text, num_merges=22)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"moqDtTzIb-NG"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens: defaultdict(<class 'int'>, {'a</w>': 1, 'sailor': 1, '</w>': 6, 'w': 3, 'e': 3, 'n': 1, 't</w>': 2, 'to</w>': 2, 'sea</w>': 6, 'see</w>': 7, 'hat</w>': 2, 'he</w>': 2, 'could</w>': 2, 'b': 3, 'u': 2, 'a': 2, 'l': 3, 't': 2, 's': 1, 'the</w>': 2, 'o': 2, 'to': 1, 'm': 1, 'f': 1, 'd': 1, 'p': 1, 'e</w>': 1})\n","Number of tokens: 27\n","Vocabulary: {'a</w>': 1, 'sailor </w>': 1, 'w e n t</w>': 1, 'to</w>': 2, 'sea</w>': 6, 'see</w>': 7, 'w hat</w>': 1, 'he</w>': 2, 'could</w>': 2, 'b u t</w>': 1, 'a l l </w>': 1, 't hat</w>': 1, 'w a s </w>': 1, 'the</w>': 2, 'b o t to m </w>': 1, 'o f </w>': 1, 'd e e p </w>': 1, 'b l u e</w>': 1}\n","Size of vocabulary: 18\n"]}],"source":["print('Tokens: {}'.format(tokens))\n","print('Number of tokens: {}'.format(len(tokens)))\n","print('Vocabulary: {}'.format(vocab))\n","print('Size of vocabulary: {}'.format(len(vocab)))"]}],"metadata":{"colab":{"provenance":[{"file_id":"1AhTv6_MqR9xkt2b8G_7bk74NA6kCk79b","timestamp":1713624911573}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
